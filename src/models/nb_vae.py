# -*- coding: utf-8 -*-
"""
è´ŸäºŒé¡¹å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆNB-VAEï¼‰

æœ¬æ¨¡å—å®ç°æ½œç©ºé—´æ¨¡å‹ï¼Œå°†é«˜ç»´å•ç»†èƒè®¡æ•°æ•°æ®æ˜ å°„åˆ°ä½ç»´æ½œç©ºé—´ã€‚

æ•°å­¦å¯¹åº”å…³ç³»ï¼š
- ç¼–ç å™¨ï¼šq_Ï†(z|x,t)ï¼Œå¯¹åº” model.md A.2èŠ‚ç¬¬38-44è¡Œ
- è§£ç å™¨ï¼šp_Ïˆ(x|z,t) = âˆ_g NB(x_g; Î¼_g, r_g)ï¼Œå¯¹åº” model.md A.2èŠ‚ç¬¬46-52è¡Œ
- ELBOæŸå¤±ï¼šL_embed = ğ”¼[log p_Ïˆ(x|z,t)] - KL(q_Ï†||p)ï¼Œå¯¹åº” model.md A.2èŠ‚ç¬¬55-65è¡Œ

å…³é”®ç‰¹æ€§ï¼š
- ä½¿ç”¨è´ŸäºŒé¡¹åˆ†å¸ƒå»ºæ¨¡å•ç»†èƒRNA-seqçš„è®¡æ•°æ•°æ®
- æ”¯æŒç»„ç»‡æ¡ä»¶è¾“å…¥ï¼ˆtissue-specificå‚æ•°ï¼‰
- æ•°å€¼ç¨³å®šæ€§ï¼šæ‰€æœ‰logè®¡ç®—æ·»åŠ epsilonï¼Œsoftplusè¾“å‡ºæ·»åŠ ä¸‹ç•Œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple

from ..config import NumericalConfig

# é»˜è®¤æ•°å€¼é…ç½®
_NUM_CFG = NumericalConfig()


class Encoder(nn.Module):
    """
    ç¼–ç å™¨ç½‘ç»œ

    å®ç° q_Ï†(z|x,t)ï¼Œå°†è§‚æµ‹xå’Œç»„ç»‡tç¼–ç ä¸ºæ½œå˜é‡zçš„åˆ†å¸ƒå‚æ•°ã€‚

    æ•°å­¦å®šä¹‰ï¼š
        q_Ï†(z|x,t) ~ N(Î¼(x,t), diag(ÏƒÂ²(x,t)))
        å…¶ä¸­ Î¼, log(ÏƒÂ²) ç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–

    å¯¹åº”ï¼šmodel.md A.2èŠ‚ï¼Œç¬¬38-44è¡Œ

    å‚æ•°:
        n_genes: åŸºå› æ•°é‡ G
        latent_dim: æ½œç©ºé—´ç»´åº¦ d_z
        n_tissues: ç»„ç»‡ç±»å‹æ•°é‡
        hidden_dim: éšè—å±‚ç»´åº¦

    æ¶æ„:
        input_layer: x â†’ hidden (G â†’ hidden_dim)
        æ‹¼æ¥ç»„ç»‡one-hot: [hidden, tissue_onehot]
        fc_mean: â†’ Î¼ (latent_dim)
        fc_logvar: â†’ log(ÏƒÂ²) (latent_dim)

    ç¤ºä¾‹:
        >>> encoder = Encoder(n_genes=2000, latent_dim=32, n_tissues=3)
        >>> x = torch.randn(64, 2000)  # (batch, genes)
        >>> tissue_onehot = torch.zeros(64, 3)
        >>> tissue_onehot[:, 0] = 1  # ç¬¬ä¸€ç§ç»„ç»‡
        >>> mu, logvar = encoder(x, tissue_onehot)
        >>> print(mu.shape, logvar.shape)
        torch.Size([64, 32]) torch.Size([64, 32])
    """

    def __init__(
        self,
        n_genes: int,
        latent_dim: int,
        n_tissues: int,
        hidden_dim: int = 512
    ):
        super().__init__()
        self.n_genes = n_genes
        self.latent_dim = latent_dim
        self.n_tissues = n_tissues
        self.hidden_dim = hidden_dim

        # è¾“å…¥å±‚ï¼šåŸºå› è¡¨è¾¾ â†’ éšè—å±‚
        self.input_layer = nn.Linear(n_genes, hidden_dim)

        # è¾“å‡ºå±‚ï¼š[éšè—å±‚ + ç»„ç»‡] â†’ æ½œç©ºé—´å‚æ•°
        self.fc_mean = nn.Linear(hidden_dim + n_tissues, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim + n_tissues, latent_dim)

    def forward(
        self,
        x: torch.Tensor,
        tissue_onehot: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            x: (B, G) åŸºå› è¡¨è¾¾è®¡æ•°ï¼Œé€šå¸¸ç»è¿‡log1på’Œæ ‡å‡†åŒ–
            tissue_onehot: (B, n_tissues) ç»„ç»‡ç±»å‹one-hotç¼–ç 

        è¿”å›:
            mu: (B, latent_dim) æ½œå˜é‡å‡å€¼
            logvar: (B, latent_dim) æ½œå˜é‡å¯¹æ•°æ–¹å·®

        å®ç°ç»†èŠ‚:
            1. éçº¿æ€§å˜æ¢ï¼šx â†’ ReLU(Wx + b)
            2. æ‹¼æ¥ç»„ç»‡ä¿¡æ¯ï¼š[h, tissue_onehot]
            3. è¾“å‡ºå‡å€¼å’Œå¯¹æ•°æ–¹å·®ï¼ˆä¸æ˜¯æ–¹å·®ï¼Œç”¨äºæ•°å€¼ç¨³å®šï¼‰
        """
        # ç¼–ç åˆ°éšè—å±‚
        h = F.relu(self.input_layer(x))  # (B, hidden_dim)

        # æ‹¼æ¥ç»„ç»‡one-hot
        h_cat = torch.cat([h, tissue_onehot], dim=-1)  # (B, hidden_dim + n_tissues)

        # è¾“å‡ºæ½œå˜é‡åˆ†å¸ƒå‚æ•°
        mu = self.fc_mean(h_cat)         # (B, latent_dim)
        logvar = self.fc_logvar(h_cat)   # (B, latent_dim)

        return mu, logvar


class DecoderNB(nn.Module):
    """
    è´ŸäºŒé¡¹è§£ç å™¨

    å®ç° p_Ïˆ(x|z,t) = âˆ_g NB(x_g; Î¼_g(z,t), r_g(t))

    æ•°å­¦å®šä¹‰ï¼š
        Î¼_g(z,t) = softplus(w_g^T z + b_{g,t}) + Îµ
        r_g(t) = exp(log_r_g)ï¼ŒåŸºå› ç‰¹å¼‚çš„ç¦»æ•£åº¦å‚æ•°

    å¯¹åº”ï¼šmodel.md A.2èŠ‚ï¼Œç¬¬46-52è¡Œ

    å‚æ•°:
        n_genes: åŸºå› æ•°é‡ G
        latent_dim: æ½œç©ºé—´ç»´åº¦ d_z
        n_tissues: ç»„ç»‡ç±»å‹æ•°é‡
        hidden_dim: éšè—å±‚ç»´åº¦

    æ¶æ„:
        fc: [z, tissue_onehot] â†’ hidden
        fc_mu: hidden â†’ Î¼ (n_genes)
        log_dispersion: å¯å­¦ä¹ å‚æ•°ï¼Œshape (n_genes,)

    å…³é”®å®ç°:
        - ä½¿ç”¨softplusæ¿€æ´»å‡½æ•°ä¿è¯Î¼ > 0
        - æ·»åŠ epsilon (1e-8) é¿å…Î¼=0å¯¼è‡´log(0)
        - ç¦»æ•£åº¦å‚æ•° r é€šè¿‡ exp(log_r) ä¿è¯ > 0

    ç¤ºä¾‹:
        >>> decoder = DecoderNB(n_genes=2000, latent_dim=32, n_tissues=3)
        >>> z = torch.randn(64, 32)
        >>> tissue_onehot = torch.zeros(64, 3)
        >>> tissue_onehot[:, 0] = 1
        >>> mu, r = decoder(z, tissue_onehot)
        >>> print(mu.shape, r.shape)
        torch.Size([64, 2000]) torch.Size([1, 2000])
    """

    def __init__(
        self,
        n_genes: int,
        latent_dim: int,
        n_tissues: int,
        hidden_dim: int = 512
    ):
        super().__init__()
        self.n_genes = n_genes
        self.latent_dim = latent_dim
        self.n_tissues = n_tissues
        self.hidden_dim = hidden_dim

        # è§£ç ç½‘ç»œï¼š[z + ç»„ç»‡] â†’ éšè—å±‚ â†’ åŸºå› è¡¨è¾¾
        self.fc = nn.Linear(latent_dim + n_tissues, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, n_genes)

        # åŸºå› ç‰¹å¼‚çš„ç¦»æ•£åº¦å‚æ•°ï¼ˆgene-wise dispersionï¼‰
        # åˆå§‹åŒ–ä¸º0ï¼Œå¯¹åº”r=1ï¼ˆæ³Šæ¾åˆ†å¸ƒçš„èµ·ç‚¹ï¼‰
        self.log_dispersion = nn.Parameter(torch.zeros(n_genes))

    def forward(
        self,
        z: torch.Tensor,
        tissue_onehot: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            z: (B, latent_dim) æ½œå˜é‡
            tissue_onehot: (B, n_tissues) ç»„ç»‡ç±»å‹one-hotç¼–ç 

        è¿”å›:
            mu: (B, G) è´ŸäºŒé¡¹åˆ†å¸ƒçš„å‡å€¼å‚æ•°
            r: (1, G) è´ŸäºŒé¡¹åˆ†å¸ƒçš„ç¦»æ•£åº¦å‚æ•°ï¼ˆåŸºå› ç‰¹å¼‚ï¼Œä¸ä¾èµ–æ ·æœ¬ï¼‰

        å®ç°ç»†èŠ‚:
            1. æ‹¼æ¥zå’Œç»„ç»‡ä¿¡æ¯
            2. é€šè¿‡éšè—å±‚
            3. è¾“å‡ºÎ¼ = softplus(...) + Îµ
            4. r = exp(log_r)ï¼ˆåŸºå› ç‰¹å¼‚ï¼Œå¹¿æ’­åˆ°batchï¼‰

        æ•°å€¼ç¨³å®šæ€§:
            - softplusè‡ªç„¶ä¿è¯è¾“å‡º>0
            - é¢å¤–æ·»åŠ 1e-8é¿å…æç«¯æƒ…å†µä¸‹Î¼=0
            - ré€šè¿‡exp(log_r)ä¿è¯>0ï¼Œlog_rå¯å­¦ä¹ 
        """
        # è§£ç åˆ°éšè—å±‚
        h = F.relu(self.fc(torch.cat([z, tissue_onehot], dim=-1)))  # (B, hidden_dim)

        # è¾“å‡ºè´ŸäºŒé¡¹åˆ†å¸ƒçš„å‡å€¼å‚æ•°Î¼
        # softplus(x) = log(1 + exp(x)) ä¿è¯è¾“å‡º>0
        mu = F.softplus(self.fc_mu(h)) + _NUM_CFG.eps_model_output  # (B, G)

        # ç¦»æ•£åº¦å‚æ•°rï¼ˆåŸºå› ç‰¹å¼‚ï¼‰
        # shape: (1, G) ä¼šè‡ªåŠ¨å¹¿æ’­åˆ° (B, G)
        # æ·»åŠ ä¸‹ç•Œé˜²æ­¢rè¿‡å°å¯¼è‡´æ•°å€¼ä¸ç¨³å®š
        r = torch.exp(self.log_dispersion).unsqueeze(0) + _NUM_CFG.eps_model_output  # (1, G)

        return mu, r


def sample_z(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
    """
    é‡å‚æ•°åŒ–é‡‡æ ·ï¼ˆreparameterization trickï¼‰

    å®ç°ï¼šz = Î¼ + Ïƒ âŠ™ Îµï¼Œå…¶ä¸­ Îµ ~ N(0,I)

    æ•°å­¦ä¾æ®ï¼š
        å¦‚æœ z ~ N(Î¼, ÏƒÂ²)ï¼Œå¯ä»¥è¡¨ç¤ºä¸ºï¼š
        z = Î¼ + Ïƒ * Îµï¼ŒÎµ ~ N(0,1)
        è¿™æ ·æ¢¯åº¦å¯ä»¥é€šè¿‡Î¼å’ŒÏƒåå‘ä¼ æ’­

    å‚æ•°:
        mu: (B, latent_dim) å‡å€¼
        logvar: (B, latent_dim) å¯¹æ•°æ–¹å·®

    è¿”å›:
        z: (B, latent_dim) é‡‡æ ·çš„æ½œå˜é‡

    ç¤ºä¾‹:
        >>> mu = torch.zeros(64, 32)
        >>> logvar = torch.zeros(64, 32)  # log(1) = 0
        >>> z = sample_z(mu, logvar)
        >>> print(z.std())  # åº”è¯¥æ¥è¿‘1
        tensor(1.0123)
    """
    # Ïƒ = exp(0.5 * log(ÏƒÂ²)) = exp(log(Ïƒ)) = Ïƒ
    # è£å‰ªlogvaré˜²æ­¢æŒ‡æ•°æº¢å‡ºï¼šlogvarâˆˆ[-10,10] â†’ stdâˆˆ[0.0067, 148.4]
    # è¿™ç¡®ä¿æ•°å€¼ç¨³å®šæ€§ï¼ŒåŒæ—¶å…è®¸è¶³å¤Ÿçš„æ–¹å·®èŒƒå›´
    logvar = torch.clamp(logvar, min=-10.0, max=10.0)
    std = torch.exp(0.5 * logvar)  # (B, latent_dim)

    # Îµ ~ N(0,1)
    eps = torch.randn_like(std)    # (B, latent_dim)

    # z = Î¼ + Ïƒ âŠ™ Îµ
    return mu + eps * std


def nb_log_likelihood(
    x: torch.Tensor,
    mu: torch.Tensor,
    r: torch.Tensor,
    eps: float = None
) -> torch.Tensor:
    """
    è´ŸäºŒé¡¹åˆ†å¸ƒçš„å¯¹æ•°ä¼¼ç„¶

    æ•°å­¦å®šä¹‰ï¼š
        NB(x; Î¼, r) = Î“(x+r) / (Î“(r) Â· x!) Â· (r/(r+Î¼))^r Â· (Î¼/(r+Î¼))^x

        log p(x) = log Î“(x+r) - log Î“(r) - log Î“(x+1)
                   + rÂ·log(r/(r+Î¼)) + xÂ·log(Î¼/(r+Î¼))

    å¯¹åº”ï¼šmodel.md A.2èŠ‚ï¼Œè´ŸäºŒé¡¹pmfå®šä¹‰

    å‚æ•°:
        x: (B, G) è§‚æµ‹è®¡æ•°
        mu: (B, G) å‡å€¼å‚æ•°
        r: (1, G) æˆ– (B, G) ç¦»æ•£åº¦å‚æ•°
        eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°

    è¿”å›:
        log_p: (B,) æ¯ä¸ªæ ·æœ¬çš„å¯¹æ•°ä¼¼ç„¶ï¼ˆå¯¹åŸºå› æ±‚å’Œï¼‰

    å®ç°ç»†èŠ‚:
        - ä½¿ç”¨torch.lgammaè®¡ç®—log Î“(x)
        - æ‰€æœ‰logè®¡ç®—æ·»åŠ epsiloné¿å…log(0)
        - è¿”å›shape (B,) è€Œé (B, G)ï¼Œå› ä¸ºå·²å¯¹åŸºå› æ±‚å’Œ

    æ•°å€¼ç¨³å®šæ€§:
        - log(r/(r+Î¼)) = log(r) - log(r+Î¼)
        - æ·»åŠ epsé¿å…Î¼=0æ—¶log(0)

    ç¤ºä¾‹:
        >>> x = torch.tensor([[5.0, 10.0]])
        >>> mu = torch.tensor([[5.0, 10.0]])
        >>> r = torch.tensor([[1.0, 1.0]])
        >>> log_p = nb_log_likelihood(x, mu, r)
        >>> print(log_p.shape)
        torch.Size([1])
    """
    # ä½¿ç”¨é…ç½®çš„epsilonå€¼
    if eps is None:
        eps = _NUM_CFG.eps_log

    x = x.float()  # ç¡®ä¿ä¸ºfloatç±»å‹

    # è¾“å…¥éªŒè¯ï¼šç¡®ä¿å‚æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼Œé˜²æ­¢lgammaäº§ç”ŸNaN
    # rå¿…é¡»>0ï¼ˆè´ŸäºŒé¡¹åˆ†å¸ƒçš„å®šä¹‰åŸŸè¦æ±‚ï¼‰
    r = torch.clamp(r, min=eps)
    # xå¿…é¡»>=0ï¼ˆè®¡æ•°æ•°æ®çš„è‡ªç„¶çº¦æŸï¼‰
    x = torch.clamp(x, min=0.0)

    # log Î“(x+r) - log Î“(r) - log Î“(x+1)
    log_coef = (
        torch.lgamma(x + r)
        - torch.lgamma(r)
        - torch.lgamma(x + 1.0)
    )  # (B, G)

    # rÂ·log(r/(r+Î¼)) + xÂ·log(Î¼/(r+Î¼))
    # ä¸ºäº†æ•°å€¼ç¨³å®šï¼Œä½¿ç”¨å¯¹æ•°å‡æ³•æ€§è´¨ï¼š
    # log(a/b) = log(a) - log(b)
    # é¿å…ç›´æ¥è®¡ç®—é™¤æ³•å’Œå°æ•°
    log_r = torch.log(r + eps)
    log_mu = torch.log(mu + eps)
    log_r_plus_mu = torch.log(r + mu + eps)

    log_r_over_r_plus_mu = log_r - log_r_plus_mu     # (B, G)
    log_mu_over_r_plus_mu = log_mu - log_r_plus_mu   # (B, G)

    log_p = (
        log_coef
        + r * log_r_over_r_plus_mu
        + x * log_mu_over_r_plus_mu
    )  # (B, G)

    # å¯¹åŸºå› ç»´åº¦æ±‚å’Œï¼Œè¿”å›æ¯ä¸ªæ ·æœ¬çš„æ€»å¯¹æ•°ä¼¼ç„¶
    return log_p.sum(dim=-1)  # (B,)


class NBVAE(nn.Module):
    """
    è´ŸäºŒé¡¹å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰

    ç»„åˆEncoderå’ŒDecoderNBï¼Œå®ç°ç«¯åˆ°ç«¯çš„VAEã€‚

    æ¨¡å‹æµç¨‹ï¼š
        x, tissue â†’ Encoder â†’ (Î¼_z, logvar_z)
        â†’ sample z = Î¼_z + Ïƒ_z âŠ™ Îµ
        â†’ Decoder â†’ (Î¼_x, r_x)
        â†’ NB likelihood p(x|Î¼_x, r_x)

    æŸå¤±å‡½æ•°ï¼šELBO = ğ”¼[log p(x|z)] - KL(q(z|x)||p(z))

    å¯¹åº”ï¼šmodel.md A.2èŠ‚ï¼Œå®Œæ•´çš„æ½œç©ºé—´æ¨¡å‹

    å‚æ•°:
        n_genes: åŸºå› æ•°é‡ G
        latent_dim: æ½œç©ºé—´ç»´åº¦ d_z
        n_tissues: ç»„ç»‡ç±»å‹æ•°é‡
        hidden_dim: éšè—å±‚ç»´åº¦

    ç¤ºä¾‹:
        >>> model = NBVAE(n_genes=2000, latent_dim=32, n_tissues=3)
        >>> x = torch.randn(64, 2000)
        >>> tissue_onehot = torch.zeros(64, 3)
        >>> tissue_onehot[:, 0] = 1
        >>> z, mu_x, r_x, mu_z, logvar_z = model(x, tissue_onehot)
        >>> print(z.shape, mu_x.shape)
        torch.Size([64, 32]) torch.Size([64, 2000])
    """

    def __init__(
        self,
        n_genes: int,
        latent_dim: int,
        n_tissues: int,
        hidden_dim: int = 512
    ):
        super().__init__()
        # ä¿å­˜æ¨¡å‹é…ç½®å‚æ•°ä¸ºå®ä¾‹å±æ€§
        self.n_genes = n_genes
        self.latent_dim = latent_dim
        self.n_tissues = n_tissues
        self.hidden_dim = hidden_dim

        # åˆ›å»ºç¼–ç å™¨å’Œè§£ç å™¨
        self.encoder = Encoder(n_genes, latent_dim, n_tissues, hidden_dim)
        self.decoder = DecoderNB(n_genes, latent_dim, n_tissues, hidden_dim)

    def forward(
        self,
        x: torch.Tensor,
        tissue_onehot: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            x: (B, G) åŸºå› è¡¨è¾¾è®¡æ•°
            tissue_onehot: (B, n_tissues) ç»„ç»‡one-hotç¼–ç 

        è¿”å›:
            z: (B, latent_dim) é‡‡æ ·çš„æ½œå˜é‡
            mu_x: (B, G) é‡å»ºçš„è¡¨è¾¾å‡å€¼
            r_x: (1, G) ç¦»æ•£åº¦å‚æ•°
            mu_z: (B, latent_dim) æ½œå˜é‡å‡å€¼ï¼ˆç”¨äºè®¡ç®—KLæ•£åº¦ï¼‰
            logvar_z: (B, latent_dim) æ½œå˜é‡å¯¹æ•°æ–¹å·®ï¼ˆç”¨äºè®¡ç®—KLæ•£åº¦ï¼‰

        æµç¨‹:
            1. ç¼–ç ï¼šx â†’ (Î¼_z, Ïƒ_z)
            2. é‡‡æ ·ï¼šz ~ N(Î¼_z, Ïƒ_z)
            3. è§£ç ï¼šz â†’ (Î¼_x, r_x)
        """
        # ç¼–ç 
        mu_z, logvar_z = self.encoder(x, tissue_onehot)

        # é‡å‚æ•°åŒ–é‡‡æ ·
        z = sample_z(mu_z, logvar_z)

        # è§£ç 
        mu_x, r_x = self.decoder(z, tissue_onehot)

        return z, mu_x, r_x, mu_z, logvar_z


def elbo_loss(
    x: torch.Tensor,
    tissue_onehot: torch.Tensor,
    model: NBVAE,
    beta: float = 1.0
) -> Tuple[torch.Tensor, dict]:
    """
    ELBOæŸå¤±å‡½æ•°

    æ•°å­¦å®šä¹‰ï¼š
        L_ELBO = ğ”¼_{q(z|x)}[log p(x|z)] - Î²Â·KL(q(z|x)||p(z))
        å…¶ä¸­ p(z) = N(0,I) æ˜¯æ ‡å‡†é«˜æ–¯å…ˆéªŒ

    å¯¹åº”ï¼šmodel.md A.2èŠ‚ï¼Œç¬¬55-65è¡Œ

    å‚æ•°:
        x: (B, G) åŸºå› è¡¨è¾¾è®¡æ•°
        tissue_onehot: (B, n_tissues) ç»„ç»‡one-hotç¼–ç 
        model: NBVAEæ¨¡å‹
        beta: KLæ•£åº¦æƒé‡ï¼ˆÎ²-VAEï¼‰ï¼Œé»˜è®¤1.0

    è¿”å›:
        loss: æ ‡é‡ï¼Œè´ŸELBOï¼ˆéœ€è¦æœ€å°åŒ–ï¼‰
        loss_dict: æŸå¤±åˆ†é‡å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
            - "recon_loss": é‡å»ºæŸå¤±ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰
            - "kl_loss": KLæ•£åº¦
            - "z": é‡‡æ ·çš„æ½œå˜é‡ï¼ˆdetachedï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼‰

    ELBOåˆ†è§£ï¼š
        - é‡å»ºé¡¹ï¼šlog p(x|z) = Î£_g log NB(x_g; Î¼_g, r_g)
        - KLæ•£åº¦é¡¹ï¼šKL(q(z|x)||N(0,I))
                  = -0.5 * Î£_d (1 + log ÏƒÂ²_d - Î¼Â²_d - ÏƒÂ²_d)

    å®ç°ç»†èŠ‚ï¼š
        - è¿”å› -ELBOï¼Œå› ä¸ºä¼˜åŒ–å™¨æ‰§è¡Œæœ€å°åŒ–
        - loss_dictä¸­çš„å„åˆ†é‡éƒ½å·²detachï¼Œç”¨äºè®°å½•å’Œç›‘æ§

    ç¤ºä¾‹:
        >>> model = NBVAE(n_genes=2000, latent_dim=32, n_tissues=3)
        >>> x = torch.randn(64, 2000)
        >>> tissue_onehot = torch.zeros(64, 3)
        >>> tissue_onehot[:, 0] = 1
        >>> loss, loss_dict = elbo_loss(x, tissue_onehot, model)
        >>> print(loss.shape, loss_dict.keys())
        torch.Size([]) dict_keys(['recon_loss', 'kl_loss', 'z'])
    """
    # å‰å‘ä¼ æ’­
    z, mu_x, r_x, mu_z, logvar_z = model(x, tissue_onehot)

    # é‡å»ºé¡¹ï¼šlog p(x|z)
    log_px = nb_log_likelihood(x, mu_x, r_x)  # (B,)
    recon_loss = -log_px.mean()  # è´Ÿå¯¹æ•°ä¼¼ç„¶

    # KLæ•£åº¦ï¼šKL(q(z|x)||N(0,I))
    # è§£æè§£ï¼š-0.5 * Î£ (1 + log ÏƒÂ² - Î¼Â² - ÏƒÂ²)
    # è£å‰ªlogvaré˜²æ­¢æŒ‡æ•°æº¢å‡ºï¼ˆä¸sample_zä¸­çš„é™åˆ¶ä¸€è‡´ï¼‰
    logvar_z_clamped = torch.clamp(logvar_z, min=-10.0, max=10.0)
    kl = -0.5 * torch.sum(
        1 + logvar_z_clamped - mu_z.pow(2) - logvar_z_clamped.exp(),
        dim=-1
    )  # (B,)
    kl_loss = kl.mean()

    # æ€»æŸå¤±ï¼šé‡å»ºæŸå¤± + Î²Â·KLæ•£åº¦
    loss = recon_loss + beta * kl_loss

    # è¿”å›æŸå¤±å’Œåˆ†é‡å­—å…¸
    loss_dict = {
        "recon_loss": recon_loss.detach(),
        "kl_loss": kl_loss.detach(),
        "z": z.detach()  # ç”¨äºä¸‹æ¸¸ä»»åŠ¡
    }

    return loss, loss_dict
