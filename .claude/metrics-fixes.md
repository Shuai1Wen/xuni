# src/evaluation/metrics.py ä¿®å¤æ–¹æ¡ˆ

## ä¿®å¤1: comprehensive_evaluation - æ¥å£ä¸åŒ¹é…ï¼ˆä¸¥é‡ï¼‰

### ä½ç½®ï¼šç¬¬367-394è¡Œ

### åŸå§‹ä»£ç ï¼š
```python
with torch.no_grad():
    for batch in dataloader:
        x0 = batch["x0"].to(device)
        x1 = batch["x1"].to(device)
        tissue_idx = batch["tissue_idx"].to(device)
        cond_vec = batch["cond_vec"].to(device)

        # ç¼–ç  x0 â†’ z0
        mu0, _ = vae_model.encoder(x0, tissue_idx)  # âŒ é”™è¯¯
        z0 = mu0  # ä½¿ç”¨å‡å€¼

        # åº”ç”¨ç®—å­ z0 â†’ z1_pred
        z1_pred = operator_model(z0, tissue_idx, cond_vec)

        # è§£ç  z1_pred â†’ x1_pred
        x1_pred = vae_model.decoder.get_mean(z1_pred, tissue_idx)  # âŒ é”™è¯¯

        # çœŸå®z1ï¼ˆç”¨äºåˆ†å¸ƒæŒ‡æ ‡ï¼‰
        mu1, _ = vae_model.encoder(x1, tissue_idx)  # âŒ é”™è¯¯
        z1_true = mu1
```

### ä¿®å¤åä»£ç ï¼š
```python
import torch.nn.functional as F

with torch.no_grad():
    for batch in dataloader:
        x0 = batch["x0"].to(device)
        x1 = batch["x1"].to(device)
        tissue_idx = batch["tissue_idx"].to(device)
        cond_vec = batch["cond_vec"].to(device)

        # å°†tissue_idxè½¬æ¢ä¸ºone-hotç¼–ç 
        tissue_onehot = F.one_hot(tissue_idx, num_classes=vae_model.n_tissues).float()

        # ç¼–ç  x0 â†’ z0
        mu0, _ = vae_model.encoder(x0, tissue_onehot)  # âœ… ä¿®å¤
        z0 = mu0  # ä½¿ç”¨å‡å€¼

        # åº”ç”¨ç®—å­ z0 â†’ z1_pred
        z1_pred = operator_model(z0, tissue_idx, cond_vec)

        # è§£ç  z1_pred â†’ x1_pred
        mu_x1_pred, _ = vae_model.decoder(z1_pred, tissue_onehot)  # âœ… ä¿®å¤
        x1_pred = mu_x1_pred

        # çœŸå®z1ï¼ˆç”¨äºåˆ†å¸ƒæŒ‡æ ‡ï¼‰
        mu1, _ = vae_model.encoder(x1, tissue_onehot)  # âœ… ä¿®å¤
        z1_true = mu1
```

### ä¿®å¤ç†ç”±ï¼š
1. **encoderéœ€è¦tissue_onehot**: encoder.forwardæ¥å—(x, tissue_onehot)å‚æ•°ï¼Œä¸æ˜¯tissue_idx
2. **decoderæ²¡æœ‰get_meanæ–¹æ³•**: decoder.forwardè¿”å›(mu, r)å…ƒç»„ï¼Œéœ€è¦è§£åŒ…ç¬¬ä¸€ä¸ªå…ƒç´ 
3. **éœ€è¦å¯¼å…¥F.one_hot**: åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥

---

## ä¿®å¤2: distribution_metrics - åæ–¹å·®é™¤é›¶é£é™©ï¼ˆä¸¥é‡ï¼‰

### ä½ç½®ï¼šç¬¬136-142è¡Œ

### åŸå§‹ä»£ç ï¼š
```python
# åæ–¹å·®è·ç¦»
z_true_centered = z_true - mean_true
z_pred_centered = z_pred - mean_pred
cov_true = (z_true_centered.T @ z_true_centered) / (z_true.shape[0] - 1)
cov_pred = (z_pred_centered.T @ z_pred_centered) / (z_pred.shape[0] - 1)
cov_dist = torch.norm(cov_true - cov_pred, p='fro').item()
metrics["cov_frobenius_dist"] = cov_dist
```

### ä¿®å¤åä»£ç ï¼š
```python
# åæ–¹å·®è·ç¦»
z_true_centered = z_true - mean_true
z_pred_centered = z_pred - mean_pred

# é˜²æ­¢é™¤é›¶ï¼šå¦‚æœæ ·æœ¬æ•°ä¸º1ï¼Œåˆ†æ¯ä¼šæ˜¯0
n_true = max(z_true.shape[0] - 1, 1)
n_pred = max(z_pred.shape[0] - 1, 1)

cov_true = (z_true_centered.T @ z_true_centered) / n_true
cov_pred = (z_pred_centered.T @ z_pred_centered) / n_pred
cov_dist = torch.norm(cov_true - cov_pred, p='fro').item()
metrics["cov_frobenius_dist"] = cov_dist
```

### ä¿®å¤ç†ç”±ï¼š
- å½“batch sizeä¸º1æ—¶ï¼Œ`z_true.shape[0] - 1 = 0`ï¼Œå¯¼è‡´é™¤é›¶
- ä½¿ç”¨`max(..., 1)`ç¡®ä¿åˆ†æ¯è‡³å°‘ä¸º1

---

## ä¿®å¤3: de_gene_prediction_metrics - pseudocountæ·»åŠ æ–¹å¼ï¼ˆé‡è¦ï¼‰

### ä½ç½®ï¼šç¬¬198-205è¡Œ

### åŸå§‹ä»£ç ï¼š
```python
# è®¡ç®—log2 fold changeï¼ˆå¹³å‡acrossç»†èƒï¼‰
# æ·»åŠ pseudocounté¿å…log(0)
mean_x0 = x0_np.mean(axis=0) + eps
mean_x1_true = x1_true_np.mean(axis=0) + eps
mean_x1_pred = x1_pred_np.mean(axis=0) + eps

log2fc_true = np.log2(mean_x1_true / mean_x0)
log2fc_pred = np.log2(mean_x1_pred / mean_x0)
```

### ä¿®å¤æ–¹æ¡ˆAï¼ˆæ¨èï¼‰ï¼šåœ¨fold changeè®¡ç®—æ—¶åŠ eps
```python
# è®¡ç®—log2 fold changeï¼ˆå¹³å‡acrossç»†èƒï¼‰
# å…ˆè®¡ç®—å‡å€¼ï¼Œç„¶ååœ¨fold changeè®¡ç®—æ—¶æ·»åŠ pseudocount
mean_x0 = x0_np.mean(axis=0)
mean_x1_true = x1_true_np.mean(axis=0)
mean_x1_pred = x1_pred_np.mean(axis=0)

# åœ¨åˆ†å­å’Œåˆ†æ¯åŒæ—¶æ·»åŠ epsï¼Œé¿å…log(0)å’Œé™¤é›¶
log2fc_true = np.log2((mean_x1_true + eps) / (mean_x0 + eps))
log2fc_pred = np.log2((mean_x1_pred + eps) / (mean_x0 + eps))
```

### ä¿®å¤æ–¹æ¡ˆBï¼ˆæ›¿ä»£ï¼‰ï¼šä½¿ç”¨maximumä¿è¯æœ€å°å€¼
```python
# è®¡ç®—log2 fold changeï¼ˆå¹³å‡acrossç»†èƒï¼‰
# ä½¿ç”¨maximumç¡®ä¿å‡å€¼ä¸å°äºeps
mean_x0 = np.maximum(x0_np.mean(axis=0), eps)
mean_x1_true = np.maximum(x1_true_np.mean(axis=0), eps)
mean_x1_pred = np.maximum(x1_pred_np.mean(axis=0), eps)

log2fc_true = np.log2(mean_x1_true / mean_x0)
log2fc_pred = np.log2(mean_x1_pred / mean_x0)
```

### ä¿®å¤ç†ç”±ï¼š
- åŸå§‹ä»£ç ï¼š`mean + eps` ä¼šå¼•å…¥biasï¼ˆå°¤å…¶å¯¹äºæ¥è¿‘0ä½†é0çš„å€¼ï¼‰
- æ–¹æ¡ˆAï¼šåœ¨æ¯”å€¼è®¡ç®—æ—¶åŠ epsï¼Œä¿æŒæ¯”ä¾‹å…³ç³»
- æ–¹æ¡ˆBï¼šç¡®ä¿æœ€å°å€¼ï¼Œæ›´ç®€æ´

---

## ä¿®å¤4: reconstruction_metrics - RÂ²è®¡ç®—è¯­ä¹‰ä¸æ¸…æ™°ï¼ˆæ”¹è¿›ï¼‰

### ä½ç½®ï¼šç¬¬78-81è¡Œ

### åŸå§‹ä»£ç ï¼š
```python
# RÂ² score
ss_res = ((x_true - x_pred) ** 2).sum()
ss_tot = ((x_true - x_true.mean()) ** 2).sum()
r2 = float(1 - ss_res / (ss_tot + 1e-8))
```

### ä¿®å¤æ–¹æ¡ˆAï¼šæ˜ç¡®æ³¨é‡Šå½“å‰æ˜¯å…¨å±€RÂ²
```python
# RÂ² scoreï¼ˆå…¨å±€ï¼šæ‰€æœ‰æ ·æœ¬å’ŒåŸºå› çš„æ€»ä½“æ‹Ÿåˆåº¦ï¼‰
# æ³¨æ„ï¼šå…¨å±€RÂ²ä¼šè¢«é«˜è¡¨è¾¾åŸºå› ä¸»å¯¼
ss_res = ((x_true - x_pred) ** 2).sum()
ss_tot = ((x_true - x_true.mean()) ** 2).sum()
r2 = float(1 - ss_res / (ss_tot + 1e-8))
```

### ä¿®å¤æ–¹æ¡ˆBï¼ˆæ¨èï¼‰ï¼šæ”¹ä¸ºper-gene RÂ²çš„ç»Ÿè®¡é‡
```python
# RÂ² scoreï¼ˆper-geneï¼Œç„¶åå–ç»Ÿè®¡é‡ï¼‰
# æ¯ä¸ªåŸºå› å•ç‹¬è®¡ç®—RÂ²ï¼Œç„¶åå–å‡å€¼å’Œä¸­ä½æ•°
ss_res_per_gene = ((x_true - x_pred) ** 2).sum(dim=0)  # (G,)
ss_tot_per_gene = ((x_true - x_true.mean(dim=0, keepdim=True)) ** 2).sum(dim=0)  # (G,)
r2_per_gene = 1 - ss_res_per_gene / (ss_tot_per_gene + 1e-8)  # (G,)

# è¿”å›å­—å…¸ä¸­æ·»åŠ ï¼š
return {
    "mse": mse,
    "mae": mae,
    "pearson_mean": float(np.mean(pearson_corrs)) if pearson_corrs else 0.0,
    "pearson_median": float(np.median(pearson_corrs)) if pearson_corrs else 0.0,
    "spearman_mean": float(np.mean(spearman_corrs)) if spearman_corrs else 0.0,
    "r2_score_mean": float(r2_per_gene.mean()),  # æ–°å¢
    "r2_score_median": float(r2_per_gene.median()),  # æ–°å¢
}
```

### ä¿®å¤ç†ç”±ï¼š
- æ–¹æ¡ˆAï¼šä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†æ˜ç¡®è¯­ä¹‰
- æ–¹æ¡ˆBï¼šæ›´ç¬¦åˆç”Ÿç‰©å­¦ç›´è§‰ï¼ˆæ¯ä¸ªåŸºå› ç‹¬ç«‹è¯„ä¼°ï¼‰

---

## ä¿®å¤5: æ·»åŠ è¾“å…¥ç»´åº¦éªŒè¯ï¼ˆæ”¹è¿›ï¼‰

### åœ¨æ¯ä¸ªå‡½æ•°å¼€å¤´æ·»åŠ éªŒè¯

#### reconstruction_metrics
```python
def reconstruction_metrics(
    x_true: torch.Tensor,
    x_pred: torch.Tensor
) -> Dict[str, float]:
    """..."""
    # è¾“å…¥éªŒè¯
    assert x_true.dim() == 2, f"x_trueåº”ä¸º2Då¼ é‡ (B, G)ï¼Œå®é™…ä¸º{x_true.dim()}D"
    assert x_pred.dim() == 2, f"x_predåº”ä¸º2Då¼ é‡ (B, G)ï¼Œå®é™…ä¸º{x_pred.dim()}D"
    assert x_true.shape == x_pred.shape, \
        f"x_trueå’Œx_predç»´åº¦ä¸åŒ¹é…ï¼š{x_true.shape} vs {x_pred.shape}"

    # ... åŸæœ‰ä»£ç 
```

#### distribution_metrics
```python
def distribution_metrics(
    z_true: torch.Tensor,
    z_pred: torch.Tensor,
    use_energy_distance: bool = True
) -> Dict[str, float]:
    """..."""
    # è¾“å…¥éªŒè¯
    assert z_true.dim() == 2, f"z_trueåº”ä¸º2Då¼ é‡ (n, d)ï¼Œå®é™…ä¸º{z_true.dim()}D"
    assert z_pred.dim() == 2, f"z_predåº”ä¸º2Då¼ é‡ (m, d)ï¼Œå®é™…ä¸º{z_pred.dim()}D"
    assert z_true.shape[1] == z_pred.shape[1], \
        f"z_trueå’Œz_predçš„ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼š{z_true.shape[1]} vs {z_pred.shape[1]}"

    # ... åŸæœ‰ä»£ç 
```

#### de_gene_prediction_metrics
```python
def de_gene_prediction_metrics(
    x0: torch.Tensor,
    x1_true: torch.Tensor,
    x1_pred: torch.Tensor,
    top_k: int = 200,
    eps: float = 1e-8
) -> Dict[str, float]:
    """..."""
    # è¾“å…¥éªŒè¯
    assert x0.dim() == 2, f"x0åº”ä¸º2Då¼ é‡ (B, G)ï¼Œå®é™…ä¸º{x0.dim()}D"
    assert x1_true.dim() == 2, f"x1_trueåº”ä¸º2Då¼ é‡ (B, G)ï¼Œå®é™…ä¸º{x1_true.dim()}D"
    assert x1_pred.dim() == 2, f"x1_predåº”ä¸º2Då¼ é‡ (B, G)ï¼Œå®é™…ä¸º{x1_pred.dim()}D"
    assert x0.shape == x1_true.shape == x1_pred.shape, \
        f"x0, x1_true, x1_predç»´åº¦ä¸åŒ¹é…ï¼š{x0.shape}, {x1_true.shape}, {x1_pred.shape}"

    B, G = x0.shape
    assert top_k <= G, f"top_k ({top_k}) ä¸èƒ½å¤§äºåŸºå› æ•°é‡ ({G})"

    # ... åŸæœ‰ä»£ç 
```

---

## å®Œæ•´ä¿®å¤åçš„æ–‡ä»¶å¯¼å…¥éƒ¨åˆ†

### åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¿…è¦çš„å¯¼å…¥ï¼š

```python
# -*- coding: utf-8 -*-
"""
è¯„ä¼°æŒ‡æ ‡æ¨¡å—
...
"""

import torch
import torch.nn.functional as F  # â† æ–°å¢ï¼šç”¨äºone-hotç¼–ç 
import numpy as np
from typing import Dict, Tuple, Optional
from sklearn.metrics import roc_auc_score, average_precision_score
from scipy.stats import spearmanr, pearsonr
from ..utils.edistance import energy_distance
```

---

## æµ‹è¯•å»ºè®®

### æµ‹è¯•1: éªŒè¯æ¥å£ä¿®å¤
```python
import torch
import torch.nn.functional as F
from src.models.nb_vae import NBVAE
from src.models.operator import OperatorModel
from src.evaluation.metrics import comprehensive_evaluation

# åˆ›å»ºæ¨¡å‹
vae = NBVAE(n_genes=100, latent_dim=16, n_tissues=3)
operator = OperatorModel(latent_dim=16, n_tissues=3, n_response_bases=5, cond_dim=32)

# åˆ›å»ºæµ‹è¯•æ•°æ®
batch = {
    "x0": torch.randn(10, 100),
    "x1": torch.randn(10, 100),
    "tissue_idx": torch.randint(0, 3, (10,)),
    "cond_vec": torch.randn(10, 32)
}

# æµ‹è¯•one-hotè½¬æ¢
tissue_onehot = F.one_hot(batch["tissue_idx"], num_classes=3).float()
print(f"tissue_idx shape: {batch['tissue_idx'].shape}")
print(f"tissue_onehot shape: {tissue_onehot.shape}")

# æµ‹è¯•encoder
mu, logvar = vae.encoder(batch["x0"], tissue_onehot)
print(f"Encoder output: mu={mu.shape}, logvar={logvar.shape}")

# æµ‹è¯•decoder
z = torch.randn(10, 16)
mu_x, r = vae.decoder(z, tissue_onehot)
print(f"Decoder output: mu_x={mu_x.shape}, r={r.shape}")
```

### æµ‹è¯•2: éªŒè¯åæ–¹å·®é™¤é›¶ä¿®å¤
```python
from src.evaluation.metrics import distribution_metrics

# æµ‹è¯•å•æ ·æœ¬æƒ…å†µ
z_true = torch.randn(1, 32)
z_pred = torch.randn(1, 32)
metrics = distribution_metrics(z_true, z_pred, use_energy_distance=False)
print(f"å•æ ·æœ¬åæ–¹å·®è·ç¦»: {metrics['cov_frobenius_dist']}")
assert not np.isnan(metrics['cov_frobenius_dist']), "åæ–¹å·®è®¡ç®—åº”è¯¥ä¸è¿”å›NaN"

# æµ‹è¯•æ­£å¸¸æƒ…å†µ
z_true = torch.randn(100, 32)
z_pred = torch.randn(100, 32)
metrics = distribution_metrics(z_true, z_pred, use_energy_distance=False)
print(f"æ­£å¸¸æ ·æœ¬åæ–¹å·®è·ç¦»: {metrics['cov_frobenius_dist']}")
```

### æµ‹è¯•3: éªŒè¯pseudocountä¿®å¤
```python
from src.evaluation.metrics import de_gene_prediction_metrics

# åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆåŒ…å«é›¶å€¼åŸºå› ï¼‰
x0 = torch.randn(100, 50).abs()
x0[:, 0] = 0  # ç¬¬ä¸€ä¸ªåŸºå› åœ¨æ‰€æœ‰æ ·æœ¬ä¸­ä¸º0

x1_true = x0 + torch.randn(100, 50) * 0.1
x1_pred = x0 + torch.randn(100, 50) * 0.1

metrics = de_gene_prediction_metrics(x0, x1_true, x1_pred, top_k=10)
print(f"DE metrics: {metrics}")
assert not np.isnan(metrics['mean_log2fc_corr']), "log2FCè®¡ç®—åº”è¯¥ä¸è¿”å›NaN"
```

---

## ä¿®å¤åº”ç”¨é¡ºåº

1. **å¿…é¡»ç«‹å³ä¿®å¤ï¼ˆP0ï¼‰**ï¼š
   - âœ… ä¿®å¤1: comprehensive_evaluationæ¥å£ä¸åŒ¹é…
   - âœ… ä¿®å¤2: distribution_metricsåæ–¹å·®é™¤é›¶

2. **å¼ºçƒˆå»ºè®®ä¿®å¤ï¼ˆP1ï¼‰**ï¼š
   - âš ï¸ ä¿®å¤3: de_gene_prediction_metricsçš„pseudocount

3. **å»ºè®®æ”¹è¿›ï¼ˆP2ï¼‰**ï¼š
   - ğŸ“ ä¿®å¤4: reconstruction_metricsçš„RÂ²è¯­ä¹‰
   - ğŸ“ ä¿®å¤5: æ·»åŠ è¾“å…¥éªŒè¯

---

## ä¿®å¤éªŒè¯æ¸…å•

ä¿®å¤å®Œæˆåï¼Œè¯·éªŒè¯ï¼š

â–¡ comprehensive_evaluationå‡½æ•°å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œä¸æŠ›å‡ºAttributeError
â–¡ å•æ ·æœ¬batchä¸ä¼šå¯¼è‡´NaNæˆ–é™¤é›¶é”™è¯¯
â–¡ DEåŸºå› é¢„æµ‹çš„log2FCè®¡ç®—åˆç†ï¼ˆé›¶å€¼åŸºå› ä¸ä¼šdominateï¼‰
â–¡ æ‰€æœ‰å‡½æ•°éƒ½æœ‰æ¸…æ™°çš„è¯­ä¹‰æ³¨é‡Š
â–¡ è¾“å…¥éªŒè¯å¯ä»¥æ•è·å¸¸è§çš„é”™è¯¯ç”¨æ³•
