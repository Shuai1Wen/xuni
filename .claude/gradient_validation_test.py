# -*- coding: utf-8 -*-
"""
æ¢¯åº¦ä¼ æ’­éªŒè¯æµ‹è¯•

éªŒè¯spectral_penaltyå’Œelbo_lossçš„æ¢¯åº¦ä¼ æ’­æ˜¯å¦æ­£ç¡®
"""

import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.operator import OperatorModel
from src.models.nb_vae import NBVAE, elbo_loss


def test_spectral_penalty_gradient():
    """æµ‹è¯•spectral_penaltyçš„æ¢¯åº¦ä¼ æ’­"""
    print("=" * 60)
    print("æµ‹è¯•1: spectral_penaltyæ¢¯åº¦ä¼ æ’­")
    print("=" * 60)

    model = OperatorModel(
        latent_dim=16,
        n_tissues=2,
        n_response_bases=3,
        cond_dim=8
    )

    # è®¡ç®—è°±èŒƒæ•°æƒ©ç½š
    penalty = model.spectral_penalty(max_allowed=1.0, n_iterations=10)

    # éªŒè¯å¯å¾®æ€§
    print(f"âœ“ penalty.requires_grad: {penalty.requires_grad}")
    assert penalty.requires_grad, "penaltyåº”è¯¥å¯å¾®"

    # åå‘ä¼ æ’­
    penalty.backward()

    # éªŒè¯æ¢¯åº¦
    assert model.A0_tissue.grad is not None, "A0_tissueåº”è¯¥æœ‰æ¢¯åº¦"
    assert model.B.grad is not None, "Båº”è¯¥æœ‰æ¢¯åº¦"

    print(f"âœ“ A0_tissue.grad éç©º: {model.A0_tissue.grad is not None}")
    print(f"âœ“ A0_tissue.grad èŒƒæ•°: {model.A0_tissue.grad.norm():.6f}")
    print(f"âœ“ B.grad éç©º: {model.B.grad is not None}")
    print(f"âœ“ B.grad èŒƒæ•°: {model.B.grad.norm():.6f}")

    print("âœ… spectral_penaltyæ¢¯åº¦ä¼ æ’­æ­£ç¡®\n")


def test_elbo_loss_gradient():
    """æµ‹è¯•elbo_lossçš„æ¢¯åº¦ä¼ æ’­"""
    print("=" * 60)
    print("æµ‹è¯•2: elbo_lossæ¢¯åº¦ä¼ æ’­")
    print("=" * 60)

    model = NBVAE(
        n_genes=100,
        latent_dim=16,
        n_tissues=2,
        hidden_dim=64
    )

    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 8
    x = torch.randn(batch_size, 100).abs() * 10  # æ¨¡æ‹Ÿè®¡æ•°æ•°æ®
    tissue_onehot = torch.zeros(batch_size, 2)
    tissue_onehot[:, 0] = 1

    # è®¡ç®—æŸå¤±
    loss, loss_dict = elbo_loss(x, tissue_onehot, model, beta=1.0)

    # éªŒè¯è¿”å›å€¼
    print(f"âœ“ loss.requires_grad: {loss.requires_grad}")
    print(f"âœ“ loss_dict['recon_loss'].requires_grad: {loss_dict['recon_loss'].requires_grad}")
    print(f"âœ“ loss_dict['kl_loss'].requires_grad: {loss_dict['kl_loss'].requires_grad}")
    print(f"âœ“ loss_dict['z'].requires_grad: {loss_dict['z'].requires_grad}")

    assert loss.requires_grad, "lossåº”è¯¥å¯å¾®"
    assert not loss_dict['recon_loss'].requires_grad, "è®°å½•å€¼åº”è¯¥detached"
    assert not loss_dict['kl_loss'].requires_grad, "è®°å½•å€¼åº”è¯¥detached"
    assert not loss_dict['z'].requires_grad, "è®°å½•å€¼åº”è¯¥detached"

    # åå‘ä¼ æ’­
    loss.backward()

    # éªŒè¯æ¢¯åº¦
    has_grad = False
    for name, param in model.named_parameters():
        if param.grad is not None:
            has_grad = True
            print(f"âœ“ {name} æœ‰æ¢¯åº¦ï¼ŒèŒƒæ•°: {param.grad.norm():.6f}")

    assert has_grad, "è‡³å°‘æœ‰ä¸€ä¸ªå‚æ•°åº”è¯¥æœ‰æ¢¯åº¦"
    print("âœ… elbo_lossæ¢¯åº¦ä¼ æ’­æ­£ç¡®\n")


def test_compute_operator_norm_no_grad():
    """æµ‹è¯•compute_operator_normä¸äº§ç”Ÿæ¢¯åº¦"""
    print("=" * 60)
    print("æµ‹è¯•3: compute_operator_normä¸äº§ç”Ÿæ¢¯åº¦ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰")
    print("=" * 60)

    model = OperatorModel(
        latent_dim=16,
        n_tissues=2,
        n_response_bases=3,
        cond_dim=8
    )

    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 4
    tissue_idx = torch.zeros(batch_size, dtype=torch.long)
    cond_vec = torch.randn(batch_size, 8)

    # è®¡ç®—èŒƒæ•°
    norms = model.compute_operator_norm(
        tissue_idx, cond_vec,
        norm_type="spectral",
        n_iterations=10
    )

    # éªŒè¯ä¸å¯å¾®ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰
    print(f"âœ“ norms.requires_grad: {norms.requires_grad}")
    assert not norms.requires_grad, "normsä¸åº”è¯¥å¯å¾®ï¼ˆå› ä¸º@torch.no_grad()ï¼‰"

    # éªŒè¯æ— æ³•åå‘ä¼ æ’­ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰
    try:
        norms.sum().backward()
        print("âŒ ä¸åº”è¯¥èƒ½å¤Ÿåå‘ä¼ æ’­")
        assert False, "normsåº”è¯¥æ— æ³•åå‘ä¼ æ’­"
    except RuntimeError as e:
        print(f"âœ“ é¢„æœŸçš„é”™è¯¯: {str(e)[:50]}...")
        print("âœ… compute_operator_normæ­£ç¡®åœ°ä½¿ç”¨@torch.no_grad()\n")


def test_spectral_penalty_vs_compute_operator_norm():
    """å¯¹æ¯”spectral_penaltyå’Œcompute_operator_normçš„åŒºåˆ«"""
    print("=" * 60)
    print("æµ‹è¯•4: spectral_penalty vs compute_operator_normèŒè´£å¯¹æ¯”")
    print("=" * 60)

    model = OperatorModel(
        latent_dim=16,
        n_tissues=2,
        n_response_bases=3,
        cond_dim=8
    )

    batch_size = 4
    tissue_idx = torch.zeros(batch_size, dtype=torch.long)
    cond_vec = torch.randn(batch_size, 8)

    # spectral_penalty: ç”¨äºè®­ç»ƒ
    penalty = model.spectral_penalty(max_allowed=1.05, n_iterations=5)
    print(f"spectral_penalty:")
    print(f"  - requires_grad: {penalty.requires_grad} (åº”è¯¥ä¸ºTrueï¼Œç”¨äºæŸå¤±è®¡ç®—)")
    print(f"  - ç”¨é€”: è®­ç»ƒæ—¶çš„ç¨³å®šæ€§æ­£åˆ™åŒ–")
    print(f"  - å€¼: {penalty.item():.6f}")

    # compute_operator_norm: ç”¨äºç›‘æ§
    norms = model.compute_operator_norm(
        tissue_idx, cond_vec,
        norm_type="spectral",
        n_iterations=10
    )
    print(f"\ncompute_operator_norm:")
    print(f"  - requires_grad: {norms.requires_grad} (åº”è¯¥ä¸ºFalseï¼Œç”¨äºç›‘æ§)")
    print(f"  - ç”¨é€”: éªŒè¯/æµ‹è¯•æ—¶çš„èŒƒæ•°ç›‘æ§")
    print(f"  - å€¼: mean={norms.mean().item():.6f}, max={norms.max().item():.6f}")

    print("\nâœ… èŒè´£åˆ†ç¦»æ¸…æ™°\n")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸ§ª æ¢¯åº¦ä¼ æ’­éªŒè¯æµ‹è¯•å¥—ä»¶".center(60, "="))
    print()

    try:
        test_spectral_penalty_gradient()
        test_elbo_loss_gradient()
        test_compute_operator_norm_no_grad()
        test_spectral_penalty_vs_compute_operator_norm()

        print("=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¢¯åº¦ä¼ æ’­å®Œå…¨æ­£ç¡®".center(60))
        print("=" * 60)

    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"\nâŒ æ„å¤–é”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
